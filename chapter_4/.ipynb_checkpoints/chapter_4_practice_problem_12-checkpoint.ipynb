{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c98b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀를 구현해 보세요 (사이킷런은 사용하지 마세요.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927184d",
   "metadata": {},
   "source": [
    "$$\\hat p_k = \\sigma(s(\\mathbf {x}))_k = {exp(s_k(\\mathbf {x}) \\over \\sum_{j=1}^K exp(s_j(\\mathbf {x}))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc5320",
   "metadata": {},
   "source": [
    "$$s_k(\\mathbf {x}) = (\\theta^{(k)})^T \\mathbf {x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591b32d",
   "metadata": {},
   "source": [
    "$$\\hat {y} = \\underset{k} {argmax} \\ \\sigma(s(\\mathbf {x}))_k = \\underset{k} {argmax} \\ s_k(\\mathbf {x})\n",
    "= \\underset{k} {argmax} \\ ((\\theta^{(k)})^T \\mathbf {x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8faa2",
   "metadata": {},
   "source": [
    "$$J(\\Theta) = - {1 \\over m} \\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)} log(\\hat p_k^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3ba40",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\theta^{(k)}}J(\\Theta) = {1 \\over m} \\sum_{i=1}^m (\\hat {p}_k^{(i)} - y_k^{(i)}) \\mathbf {x}^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9905d1",
   "metadata": {},
   "source": [
    "$$\\theta^{(next\\;step)} = \\theta - \\eta \\nabla_\\theta MSE(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4285a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f6e01d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1b4a301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones([X.shape[0], 1]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9a27bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  1.4 0.2]\n",
      " [1.  1.4 0.2]\n",
      " [1.  1.3 0.2]\n",
      " [1.  1.5 0.2]\n",
      " [1.  1.4 0.2]\n",
      " [1.  1.7 0.4]\n",
      " [1.  1.4 0.3]\n",
      " [1.  1.5 0.2]\n",
      " [1.  1.4 0.2]\n",
      " [1.  1.5 0.1]\n",
      " [1.  1.5 0.2]\n",
      " [1.  1.6 0.2]\n",
      " [1.  1.4 0.1]\n",
      " [1.  1.1 0.1]\n",
      " [1.  1.2 0.2]\n",
      " [1.  1.5 0.4]\n",
      " [1.  1.3 0.4]\n",
      " [1.  1.4 0.3]\n",
      " [1.  1.7 0.3]\n",
      " [1.  1.5 0.3]\n",
      " [1.  1.7 0.2]\n",
      " [1.  1.5 0.4]\n",
      " [1.  1.  0.2]\n",
      " [1.  1.7 0.5]\n",
      " [1.  1.9 0.2]\n",
      " [1.  1.6 0.2]\n",
      " [1.  1.6 0.4]\n",
      " [1.  1.5 0.2]\n",
      " [1.  1.4 0.2]\n",
      " [1.  1.6 0.2]\n",
      " [1.  1.6 0.2]\n",
      " [1.  1.5 0.4]\n",
      " [1.  1.5 0.1]\n",
      " [1.  1.4 0.2]\n",
      " [1.  1.5 0.2]\n",
      " [1.  1.2 0.2]\n",
      " [1.  1.3 0.2]\n",
      " [1.  1.4 0.1]\n",
      " [1.  1.3 0.2]\n",
      " [1.  1.5 0.2]\n",
      " [1.  1.3 0.3]\n",
      " [1.  1.3 0.3]\n",
      " [1.  1.3 0.2]\n",
      " [1.  1.6 0.6]\n",
      " [1.  1.9 0.4]\n",
      " [1.  1.4 0.3]\n",
      " [1.  1.6 0.2]\n",
      " [1.  1.4 0.2]\n",
      " [1.  1.5 0.2]\n",
      " [1.  1.4 0.2]\n",
      " [1.  4.7 1.4]\n",
      " [1.  4.5 1.5]\n",
      " [1.  4.9 1.5]\n",
      " [1.  4.  1.3]\n",
      " [1.  4.6 1.5]\n",
      " [1.  4.5 1.3]\n",
      " [1.  4.7 1.6]\n",
      " [1.  3.3 1. ]\n",
      " [1.  4.6 1.3]\n",
      " [1.  3.9 1.4]\n",
      " [1.  3.5 1. ]\n",
      " [1.  4.2 1.5]\n",
      " [1.  4.  1. ]\n",
      " [1.  4.7 1.4]\n",
      " [1.  3.6 1.3]\n",
      " [1.  4.4 1.4]\n",
      " [1.  4.5 1.5]\n",
      " [1.  4.1 1. ]\n",
      " [1.  4.5 1.5]\n",
      " [1.  3.9 1.1]\n",
      " [1.  4.8 1.8]\n",
      " [1.  4.  1.3]\n",
      " [1.  4.9 1.5]\n",
      " [1.  4.7 1.2]\n",
      " [1.  4.3 1.3]\n",
      " [1.  4.4 1.4]\n",
      " [1.  4.8 1.4]\n",
      " [1.  5.  1.7]\n",
      " [1.  4.5 1.5]\n",
      " [1.  3.5 1. ]\n",
      " [1.  3.8 1.1]\n",
      " [1.  3.7 1. ]\n",
      " [1.  3.9 1.2]\n",
      " [1.  5.1 1.6]\n",
      " [1.  4.5 1.5]\n",
      " [1.  4.5 1.6]\n",
      " [1.  4.7 1.5]\n",
      " [1.  4.4 1.3]\n",
      " [1.  4.1 1.3]\n",
      " [1.  4.  1.3]\n",
      " [1.  4.4 1.2]\n",
      " [1.  4.6 1.4]\n",
      " [1.  4.  1.2]\n",
      " [1.  3.3 1. ]\n",
      " [1.  4.2 1.3]\n",
      " [1.  4.2 1.2]\n",
      " [1.  4.2 1.3]\n",
      " [1.  4.3 1.3]\n",
      " [1.  3.  1.1]\n",
      " [1.  4.1 1.3]\n",
      " [1.  6.  2.5]\n",
      " [1.  5.1 1.9]\n",
      " [1.  5.9 2.1]\n",
      " [1.  5.6 1.8]\n",
      " [1.  5.8 2.2]\n",
      " [1.  6.6 2.1]\n",
      " [1.  4.5 1.7]\n",
      " [1.  6.3 1.8]\n",
      " [1.  5.8 1.8]\n",
      " [1.  6.1 2.5]\n",
      " [1.  5.1 2. ]\n",
      " [1.  5.3 1.9]\n",
      " [1.  5.5 2.1]\n",
      " [1.  5.  2. ]\n",
      " [1.  5.1 2.4]\n",
      " [1.  5.3 2.3]\n",
      " [1.  5.5 1.8]\n",
      " [1.  6.7 2.2]\n",
      " [1.  6.9 2.3]\n",
      " [1.  5.  1.5]\n",
      " [1.  5.7 2.3]\n",
      " [1.  4.9 2. ]\n",
      " [1.  6.7 2. ]\n",
      " [1.  4.9 1.8]\n",
      " [1.  5.7 2.1]\n",
      " [1.  6.  1.8]\n",
      " [1.  4.8 1.8]\n",
      " [1.  4.9 1.8]\n",
      " [1.  5.6 2.1]\n",
      " [1.  5.8 1.6]\n",
      " [1.  6.1 1.9]\n",
      " [1.  6.4 2. ]\n",
      " [1.  5.6 2.2]\n",
      " [1.  5.1 1.5]\n",
      " [1.  5.6 1.4]\n",
      " [1.  6.1 2.3]\n",
      " [1.  5.6 2.4]\n",
      " [1.  5.5 1.8]\n",
      " [1.  4.8 1.8]\n",
      " [1.  5.4 2.1]\n",
      " [1.  5.6 2.4]\n",
      " [1.  5.1 2.3]\n",
      " [1.  5.1 1.9]\n",
      " [1.  5.9 2.3]\n",
      " [1.  5.7 2.5]\n",
      " [1.  5.2 2.3]\n",
      " [1.  5.  1.9]\n",
      " [1.  5.2 2. ]\n",
      " [1.  5.4 2.3]\n",
      " [1.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6bd64ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1f315251",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "validationn_ratio = 0.2\n",
    "total_size = len(X)\n",
    "\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validationn_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "X_valid = X[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]\n",
    "X_test = X[rnd_indices[-test_size:]]\n",
    "y_test = y[rnd_indices[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "04c6242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logit):\n",
    "    return np.exp(logit) / np.sum(np.exp(logit), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "13c8c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    y_one_hot = (y == 0)\n",
    "    for i in range(1, len(np.unique(y))):\n",
    "        y_one_hot = np.c_[y_one_hot, (y == i)]\n",
    "    \n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "17c1f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression(X, y, eta, epsilon, n_epochs):\n",
    "    Theta = np.random.rand(X.shape[1], len(np.unique(y)))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        logit = X.dot(Theta)\n",
    "        if epoch % 100 == 0:\n",
    "            loss = -np.mean(np.sum(one_hot(y) * np.log(softmax(logit) + epsilon), axis=1))\n",
    "            print(epoch, loss)\n",
    "        Theta = Theta - eta * (1 / X.shape[0]) * X.T.dot(softmax(logit) - one_hot(y))\n",
    "        \n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "23c39831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.1041298095182546\n",
      "100 0.9773253549775368\n",
      "200 0.9244827149782782\n",
      "300 0.8772659673940222\n",
      "400 0.835134154553148\n",
      "500 0.7975963791730117\n",
      "600 0.7641553224875609\n",
      "700 0.7343316692051317\n",
      "800 0.707679967227584\n",
      "900 0.6837971094678368\n",
      "1000 0.6623252726598646\n",
      "1100 0.6429511477455903\n",
      "1200 0.6254029728082421\n",
      "1300 0.6094464693370865\n",
      "1400 0.5948804114237712\n",
      "1500 0.5815322702225296\n",
      "1600 0.5692541745220324\n",
      "1700 0.5579192971369724\n",
      "1800 0.5474186970886771\n",
      "1900 0.537658602630962\n",
      "2000 0.52855809752725\n",
      "2100 0.5200471638364138\n",
      "2200 0.5120650330465584\n",
      "2300 0.5045588000737211\n",
      "2400 0.497482259219808\n",
      "2500 0.4907949263916581\n",
      "2600 0.4844612170284537\n",
      "2700 0.47844975392307776\n",
      "2800 0.47273278331190705\n",
      "2900 0.4672856812168731\n",
      "3000 0.4620865350818338\n",
      "3100 0.4571157883076949\n",
      "3200 0.45235593742176605\n",
      "3300 0.44779127338056784\n",
      "3400 0.4434076599606987\n",
      "3500 0.4391923433913653\n",
      "3600 0.4351337883693733\n",
      "3700 0.4312215364102865\n",
      "3800 0.42744608315937294\n",
      "3900 0.423798771838657\n",
      "4000 0.4202717004630981\n",
      "4100 0.4168576408369544\n",
      "4200 0.41354996765493185\n",
      "4300 0.4103425962933109\n",
      "4400 0.40722992809332115\n",
      "4500 0.40420680212026183\n",
      "4600 0.40126845253352694\n",
      "4700 0.3984104708299108\n",
      "4800 0.3956287723295474\n",
      "4900 0.3929195663639816\n",
      "5000 0.39027932970205215\n"
     ]
    }
   ],
   "source": [
    "Theta = softmax_regression(X_train, y_train, 0.01, 1e-7, 5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d7e90212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "y_predict = np.argmax(softmax(logits), axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ac48dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression(X, y, eta, alpha, epsilon, n_epochs):\n",
    "    Theta = np.random.rand(X.shape[1], len(np.unique(y)))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        logit = X.dot(Theta)\n",
    "        if epoch % 100 == 0:\n",
    "            xentropy_loss = -np.mean(np.sum(one_hot(y) * np.log(softmax(logit) + epsilon), axis=1))\n",
    "            l2_loss = (1/2) * np.sum(np.square(Theta[1:])) \n",
    "            loss = xentropy_loss + alpha * l2_loss\n",
    "            print(epoch, loss)\n",
    "        Theta = Theta - eta * ((1 / X.shape[0]) * X.T.dot(softmax(logit) - one_hot(y)) + np.r_[np.zeros([1, Theta.shape[1]]), alpha * Theta[1:]])\n",
    "        \n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e4e14fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1286867269609013\n",
      "100 0.727001520381528\n",
      "200 0.6172511945261256\n",
      "300 0.5722314799724413\n",
      "400 0.5483493547391751\n",
      "500 0.5336165724232963\n",
      "600 0.5236333077278472\n",
      "700 0.5164448128733705\n",
      "800 0.5110553848619248\n",
      "900 0.506901544340828\n",
      "1000 0.5036368734426767\n",
      "1100 0.5010341406487107\n",
      "1200 0.4989366341361362\n",
      "1300 0.4972320351851622\n",
      "1400 0.4958374238613538\n",
      "1500 0.49469016215995615\n",
      "1600 0.4937420710690712\n",
      "1700 0.4929555543676384\n",
      "1800 0.4923009279797319\n",
      "1900 0.49175452675278375\n",
      "2000 0.49129733027502115\n",
      "2100 0.4909139456052336\n",
      "2200 0.4905918416541769\n",
      "2300 0.4903207648191134\n",
      "2400 0.49009228756446294\n",
      "2500 0.4898994560562627\n",
      "2600 0.4897365126090409\n",
      "2700 0.48959867531266066\n",
      "2800 0.4894819618232197\n",
      "2900 0.48938304758370466\n",
      "3000 0.48929915110927685\n",
      "3100 0.4892279407063359\n",
      "3200 0.48916745827987496\n",
      "3300 0.4891160568470703\n",
      "3400 0.4890723491045481\n",
      "3500 0.4890351649543214\n",
      "3600 0.48900351632314565\n",
      "3700 0.4889765679438932\n",
      "3800 0.48895361302877094\n",
      "3900 0.4889340529699576\n",
      "4000 0.4889173803662827\n",
      "4100 0.48890316480451634\n",
      "4200 0.48889104092794067\n",
      "4300 0.4888806984086741\n",
      "4400 0.48887187350798567\n",
      "4500 0.4888643419638583\n",
      "4600 0.4888579129899065\n",
      "4700 0.48885242420644226\n",
      "4800 0.4888477373545906\n",
      "4900 0.4888437346691382\n",
      "5000 0.4888403158062639\n"
     ]
    }
   ],
   "source": [
    "Theta = softmax_regression(X_train, y_train, 0.1, 0.1, 1e-7, 5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "072a456e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "y_predict = np.argmax(softmax(logits), axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fe9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression(X_train, y_train, X_val, y_val, eta, alpha, epsilon, n_epochs):\n",
    "    Theta = np.random.rand(X_train.shape[1], len(np.unique(y_train)))\n",
    "    best_loss = np.infty\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        logit = X_train.dot(Theta)\n",
    "        Theta = Theta - eta * ((1 / X_train.shape[0]) * X_train.T.dot(softmax(logit) - one_hot(y_train)) + np.r_[np.zeros([1, Theta.shape[1]]), alpha * Theta[1:]])\n",
    "        \n",
    "        logit = X_valid.dot(Theta)\n",
    "        xentropy_loss = -np.mean(np.sum(one_hot(y_val) * np.log(softmax(logit) + epsilon), axis=1))\n",
    "        l2_loss = (1/2) * np.sum(np.square(Theta[1:])) \n",
    "        loss = xentropy_loss + alpha * l2_loss\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, loss)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "        else:\n",
    "            print(epoch - 1, best_loss)\n",
    "            print(epoch, loss, \"조기 종료!\")\n",
    "            return Theta\n",
    "            \n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee77f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "y_predict = np.argmax(softmax(logits), axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
